{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "clrs = np.array(['#003057', '#EAAA00', '#4B8B9B', '#B3A369', '#377117', '#1879DB', '#8E8B76', '#F5D580', '#002233', '#808080'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used in the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perovskite_data\n",
    "df = pd.read_csv('data/perovskite_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dow impurity data \n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "dow_df = df[['Date', 'y:Impurity']]\n",
    "dow_df.loc[:,'Date'] = pd.to_datetime(dow_df['Date'])\n",
    "dow_df = dow_df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_df['01/01/2016 05:00:00':'01/01/2016 12:00:00']  # select data based on dates\n",
    "dow_df.plot(); # plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data can be \"filtered\" using logical statements:\n",
    "bools = df['x1:Primary Column Reflux Flow'] > 350\n",
    "print(bools)\n",
    "df_filtered = df[bools]\n",
    "df_filtered.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get correlation matrix for the impurity data and find highly correlated features \n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (17, 15), dpi = 150)\n",
    "\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, ax = ax);\n",
    "\n",
    "corr[\"Avg_Delta_Composition Primary Column\"] > 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop values \n",
    "# Let's use another useful method called drop() to drop Avg_Delta_Composition Primary Column\n",
    "df_no_avg_delta = df_dropped_obs.drop('Avg_Delta_Composition Primary Column', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outliers from dataframe\n",
    "xi = df_dropped_obs[\"x3:Input to Primary Column Bed 3 Flow\"].copy()\n",
    "mu = np.mean(xi)\n",
    "stdev = np.std(xi)\n",
    "z_cutoff = 3\n",
    "\n",
    "zi = (xi - mu)/stdev\n",
    "xi_nooutliers = xi[np.abs(zi) < z_cutoff]\n",
    "print('Observations before removing outliers: {}'.format(xi.shape[0]))\n",
    "print('Observations after removing outliers: {}'.format(xi_nooutliers.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data/impurity_dataset-training.xlsx')\n",
    "def is_real_and_finite(x):\n",
    "    if not np.isreal(x):\n",
    "        return False\n",
    "    elif not np.isfinite(x):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "all_data = df[df.columns[1:]].values #drop the first column (date)\n",
    "numeric_map = df[df.columns[1:]].applymap(is_real_and_finite)\n",
    "real_rows = numeric_map.all(axis=1).copy().values #True if all values in a row are real numbers\n",
    "X_dow = np.array(all_data[real_rows,:-5], dtype='float') #drop the last 5 cols that are not inputs\n",
    "y_dow = np.array(all_data[real_rows,-3], dtype='float')\n",
    "y_dow = y_dow.reshape(-1,1)\n",
    "print(X_dow.shape, y_dow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical data format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install h5py\n",
    "! rm data/impurity_data.hdf5\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "f = h5py.File(\"data/impurity_data.hdf5\", \"w\") #<- the \"w\" argument tells h5py to create a new file. \"w\" stands for \"write\"\n",
    "dset = f.create_dataset(\"training\", X_dow_numbers.shape)\n",
    "dset[:, :] = X_dow_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online data access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "page = requests.get('https://pubchem.ncbi.nlm.nih.gov/compound/Ethanol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/ethanol.json') as f:\n",
    "    etoh = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILES string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES = etoh['Record']['Section'][2]['Section'][1]['Section'][3]['Information'][0]['Value']['StringWithMarkup'][0]['String']#['StringValue']\n",
    "MW = etoh['Record']['Section'][3]['Section'][0]['Section'][0]['Information'][0]['Value']['Number'][0]\n",
    "print('SMILES: {}'.format(SMILES))\n",
    "print('Molecular Weight: {}'.format(MW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the same information with significantly less effort:\n",
    "SMILES = etoh_simple['PC_Compounds'][0]['props'][18]['value']['sval']\n",
    "MW = etoh_simple['PC_Compounds'][0]['props'][17]['value']['fval']\n",
    "print('SMILES: {}'.format(SMILES))\n",
    "print('Molecular Weight: {}'.format(MW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the ethanol_simple.json file as input. You will need both bonds and atoms information. \n",
    "#Note that element refers to the atomic number (e.g. hydrogen is 1).\n",
    "\n",
    "print(etoh_simple['PC_Compounds'][0]['bonds'])\n",
    "print(etoh_simple['PC_Compounds'][0]['atoms'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application Programming Interfaces (APIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RESTful API's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubchempy as pcpy\n",
    "#help(pcpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compounds = pcpy.get_compounds('Ethanol','name')\n",
    "print(compounds)\n",
    "etoh = compounds[0]\n",
    "print(etoh.bonds[0].aid2)\n",
    "print(etoh.atoms[etoh.bonds[0].aid1].element)\n",
    "print(etoh.atoms[etoh.bonds[0].aid2].element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the PubChemPy API to ask for specific attributes with the `get_properties` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pcpy.get_properties('CanonicalSMILES', 'ethanol', 'name')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CO2 data\n",
    "import statsmodels.api as api\n",
    "\n",
    "sm_data = api.datasets.co2.load_pandas()\n",
    "co2_df = sm_data.data\n",
    "co2_df.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST dataset\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(\"Digits data shape: {}\".format(digits.data.shape))\n",
    "print(\"Digits output shape: {}\".format(digits.target.shape))\n",
    "X_mnist = np.array(digits.data)\n",
    "y_mnist = np.array(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/ethanol_IR.csv')\n",
    "x_all = df['wavenumber [cm^-1]'].values\n",
    "y_all = df['absorbance'].values\n",
    "\n",
    "x_peak = x_all[475:575]\n",
    "y_peak = y_all[475:575]\n",
    "\n",
    "m = 20\n",
    "\n",
    "x_peak = x_peak.reshape(-1, 1) #create a column vector\n",
    "X_vdm = vandermonde(x_peak, m) #generate Vandermonde matrix\n",
    "b_m = np.dot(X_vdm.T, y_peak) #generate b vector with new features\n",
    "A_m = np.dot(X_vdm.T, X_vdm) #generate A matrix with new features\n",
    "w_m = np.linalg.solve(A_m, b_m) #solve Ax=b with new features\n",
    "\n",
    "yhat_m = np.dot(X_vdm, w_m) #compute predictions\n",
    "SSE_m = np.sum((y_peak - yhat_m)**2) #compute sum of squared errors\n",
    "print('Sum of Squared Errors: {}'.format(SSE_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X.T@X\n",
    "b = X.T@y\n",
    "w_lsr = np.linalg.solve(A,b)\n",
    "\n",
    "yhat = X@w_lsr\n",
    "print('Weights from least-squares regression: {}'.format(w_lsr))\n",
    "print('Original weights to generate data: {}'.format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acurracy metrics \n",
    "#r2=(SST-SSE)/SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add intercept \n",
    "\n",
    "X[:,-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False) #create a linear regression model instance (no intercept needed)\n",
    "model.fit(X, y_peak) #fit the model\n",
    "r2 = model.score(X, y_peak) #get the \"score\", which is equivalent to r^2\n",
    "\n",
    "yhat = model.predict(X) #create the model prediction\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '.')\n",
    "ax.plot(x_peak, yhat, 'o', markerfacecolor='none')\n",
    "ax.set_xlabel('wavenumber [$cm^{-1}$]')\n",
    "ax.set_ylabel('absorbance')\n",
    "ax.set_title('IR spectra data')\n",
    "ax.legend(['Original Data', 'Linear Regression'])\n",
    "print('r^2 = {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Differentiation \n",
    "import autograd.numpy as np   # autograd has its own \"version\" of numpy that must be used\n",
    "from autograd import grad # the \"grad\" function provides derivatives\n",
    "\n",
    "def g(lamda, x=x, y=y, m=2):\n",
    "    return gaussian_loss(lamda, x, y, m)\n",
    "\n",
    "diff_g = grad(g)\n",
    "print(g(lamda))\n",
    "print(diff_g(lamda))\n",
    "diff_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "bad_guess = [0.1, 1.0, 0.5, 0.3, 0.1, 0.4]\n",
    "better_guess = [0.35, 0.75, 0.21, 0.52, 0.53, 0.11]\n",
    "guess = bad_guess\n",
    "\n",
    "N_iter = 1000\n",
    "h = 0.1\n",
    "for i in range(N_iter):\n",
    "    guess = guess - h*np.array(diff_g(guess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization with scipy \n",
    "from  scipy.optimize  import minimize\n",
    "\n",
    "result = minimize(g, bad_guess, method='BFGS')\n",
    "result\n",
    "\n",
    "result.x\n",
    "\n",
    "print('Actual Input: {}'.format(str(result.x)))\n",
    "print('Regression Result: {}'.format(str(lamda)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Regression¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf function manual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(x_train, x_test=None, gamma=1):\n",
    "    if x_test is None:\n",
    "        x_test = x_train\n",
    "    N = len(x_test) #<- number of data points\n",
    "    M = len(x_train) #<- number of features\n",
    "    X = np.zeros((N,M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            X[i,j] = np.exp(-gamma*(x_test[i] - x_train[j])**2)\n",
    "    return X\n",
    "\n",
    "sigma = 100\n",
    "gamma = 1./(2*sigma**2)\n",
    "x_test = np.linspace(min(x_peak), max(x_peak), 300)\n",
    "X_rbf = rbf(x_peak, x_test=x_test, gamma=gamma)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_test, X_rbf[:,50], '-')\n",
    "ax.set_xlabel('wavenumber [$cm^{-1}$]')\n",
    "ax.set_ylabel('absorbance')\n",
    "ax.set_title('rbf basis $\\sigma$ = {}'.format(str(sigma)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = rbf(x_peak, gamma=gamma)\n",
    "\n",
    "model_rbf = LinearRegression() #create a linear regression model instance\n",
    "model_rbf.fit(X_train, y_peak) #fit the model\n",
    "r2 = model_rbf.score(X_train, y_peak) #get the \"score\", which is equivalent to r^2\n",
    "print('r^2 = {}'.format(r2))\n",
    "\n",
    "X_test = rbf(x_peak, x_test=x_test, gamma=gamma)\n",
    "\n",
    "yhat_rbf = model_rbf.predict(X_test) #create the model prediction\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, 'o')\n",
    "ax.plot(x_test, yhat_rbf, '-', markerfacecolor='none')\n",
    "ax.set_xlabel('wavenumber [$cm^{-1}$]')\n",
    "ax.set_ylabel('absorbance')\n",
    "ax.set_title('kernel regression $\\sigma$ = {}'.format(str(sigma)))\n",
    "ax.legend(['Original Data', 'Linear Regression']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = 3\n",
    "sigma = 10\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "x_train = x_peak[::spacing]\n",
    "y_train = y_peak[::spacing]\n",
    "\n",
    "X_train = rbf(x_train, gamma=gamma)\n",
    "\n",
    "model_rbf = LinearRegression() #create a linear regression model instance\n",
    "model_rbf.fit(X_train, y_train) #fit the model\n",
    "\n",
    "r2 = model_rbf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation (test train split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_peak, y_peak, test_size=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "sigma = 100\n",
    "gamma = 1. / 2 / sigma**2\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_peak, y_peak, '-o', markerfacecolor='none')\n",
    "\n",
    "r2_test = []\n",
    "\n",
    "for train_index, test_index in kf.split(x_peak):\n",
    "    x_train, x_test = x_peak[train_index], x_peak[test_index]\n",
    "    y_train, y_test = y_peak[train_index], y_peak[test_index]\n",
    "    \n",
    "    X_train = rbf(x_train, gamma=gamma)\n",
    "\n",
    "    model_rbf = LinearRegression() #create a linear regression model instance\n",
    "    model_rbf.fit(X_train, y_train) #fit the model\n",
    "    r2 = model_rbf.score(X_train, y_train) #get the \"score\", which is equivalent to r^2\n",
    "    print('r^2 training = {}'.format(r2))\n",
    "\n",
    "    X_test = rbf(x_train, x_test=x_test, gamma=gamma)\n",
    "\n",
    "    yhat_rbf = model_rbf.predict(X_test) #create the model prediction\n",
    "\n",
    "    r2 = model_rbf.score(X_test, y_test) #get the \"score\", which is equivalent to r^2\n",
    "    print('r^2 testing = {}'.format(r2))\n",
    "    r2_test.append(r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice #<- randomly select items from a list\n",
    "\n",
    "def bootstrap_linregress(x_all, y_all, N):\n",
    "    m_list = []\n",
    "    b_list = []\n",
    "    for n in range(N):\n",
    "        subset = choice(range(len(x_all)), size=len(x_all), replace=True)\n",
    "        xprime = [x_all[j] for j in subset]\n",
    "        yprime = [y_all[j] for j in subset]\n",
    "        if np.std(xprime) > 0:\n",
    "            m, b = np.polyfit(xprime, yprime, deg=1)\n",
    "        else:\n",
    "            m = 0\n",
    "            b = np.mean(yprime)\n",
    "        \n",
    "        m_list.append(m)\n",
    "        b_list.append(b)\n",
    "    return m_list, b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "x_peak = x_peak.reshape(-1, 1)\n",
    "y_peak = y_peak.reshape(-1, 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_peak, y_peak, test_size = 0.4)\n",
    "\n",
    "gpr = GaussianProcessRegressor(kernel = RBF(1), alpha = 0.000005)\n",
    "\n",
    "gpr.fit(x_train, y_train)\n",
    "\n",
    "y_gpr, y_std = gpr.predict(x_peak, return_std = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIC \n",
    "def BIC(y, yhat, k):\n",
    "    err = y - yhat\n",
    "    sigma = np.std(np.real(err))\n",
    "    n = len(y)\n",
    "    B = n*np.log(sigma**2) + k*np.log(n)\n",
    "    return B\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def polynomial_features(x, N):\n",
    "    # function to return a matrix of polynomials for x to order N\n",
    "    # One-liner uses \"list comprehension\" to iterate through range 0 - N (note N+1 since range function is not inclusive)\n",
    "    # The input, x, is raised to the power of N for each value of N\n",
    "    # The result is converted to an array and transposed so that columns correspond to features and rows correspond to data points (individual x values)\n",
    "    return np.array([x**k for k in range(0,N)]).T\n",
    "\n",
    "N = 40\n",
    "X_poly = polynomial_features(x_peak, N)\n",
    "\n",
    "LR_poly = LinearRegression() #create a linear regression model instance\n",
    "LR_poly.fit(X_poly, y_peak) #fit the model\n",
    "yhat_poly = LR_poly.predict(X_poly)\n",
    "\n",
    "BIC_poly = BIC(y_peak, yhat_poly, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "sigma = 10\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "KRR = KernelRidge(alpha=alpha, kernel='rbf', gamma=gamma)\n",
    "x_peak = x_peak.reshape(-1,1) #we need to convert these to columns\n",
    "y_peak = y_peak.reshape(-1,1)\n",
    "\n",
    "KRR.fit(x_peak, y_peak)\n",
    "\n",
    "x_predict = np.linspace(min(x_peak), max(x_peak), 300) #create prediction data\n",
    "yhat_KRR = KRR.predict(x_predict)\n",
    "\n",
    "r2_test = KRR.score(x_test, y_test)\n",
    "\n",
    "coeffs= KRR.dual_coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "sigma = 10\n",
    "gamma = 1./(2*sigma**2)\n",
    "\n",
    "alpha = 1e-4\n",
    "\n",
    "LASSO = Lasso(alpha=alpha)\n",
    "LASSO.fit(X_train, y_train)\n",
    "print('The number of coefficients: {}'.format(len(LASSO.coef_)))\n",
    "\n",
    "x_predict = np.linspace(min(x_peak), max(x_peak), 300) #create prediction data\n",
    "X_predict = rbf_kernel(x_predict, x_train, gamma=gamma)\n",
    "\n",
    "yhat_LASSO = LASSO.predict(X_predict)\n",
    "\n",
    "\n",
    "coeffs = LASSO.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero = [f for f in np.isclose(coeffs,0) if f == False]\n",
    "print('Total number of non-zero parameters: {}'.format(len(nonzero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sigmas = np.array([5, 10, 15, 20, 25, 30,35, 40])\n",
    "gammas = 1./(2*sigmas**2)\n",
    "\n",
    "alphas = np.array([1e-9, 1e-5, 1e-4,1e-3, 1e-2,1e-1, 1])\n",
    "\n",
    "parameter_ranges = {'alpha':alphas, 'gamma':gammas}\n",
    "\n",
    "KRR = KernelRidge(kernel='rbf')\n",
    "\n",
    "KRR_search = GridSearchCV(KRR, parameter_ranges, cv=3)\n",
    "KRR_search.fit(x_train,y_train)\n",
    "KRR_search.best_estimator_, KRR_search.best_score_\n",
    "\n",
    "yhat_KRR = KRR_search.best_estimator_.predict(x_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import choice #<- randomly select items from a list\n",
    "\n",
    "def bootstrap_linregress(x_all, y_all, N):\n",
    "    m_list = []\n",
    "    b_list = []\n",
    "    for n in range(N):\n",
    "        subset = choice(range(len(x_all)), size=len(x_all), replace=True)\n",
    "        xprime = [x_all[j] for j in subset]\n",
    "        yprime = [y_all[j] for j in subset]\n",
    "        if np.std(xprime) > 0:\n",
    "            m, b = np.polyfit(xprime, yprime, deg=1)\n",
    "        else:\n",
    "            m = 0\n",
    "            b = np.mean(yprime)\n",
    "        \n",
    "        m_list.append(m)\n",
    "        b_list.append(b)\n",
    "    return m_list, b_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Dimensional Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature visulization histogram\n",
    "print('X dimensions: {}'.format(X.shape))\n",
    "print('Feature names: {}'.format(x_names))\n",
    "N = X.shape[-1]\n",
    "n = int(np.sqrt(N))\n",
    "fig, axes = plt.subplots(n, n+1, figsize = (6*n, 6*n))\n",
    "ax_list = axes.ravel()\n",
    "for i in range(N):\n",
    "    ax_list[i].hist(X[:,i])\n",
    "    ax_list[i].set_xlabel(x_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature correlations \n",
    "covar = np.cov(X.T)\n",
    "fig,ax = plt.subplots()\n",
    "c = ax.imshow(covar)\n",
    "fig.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling Features and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() \n",
    "train_std=sc.transform(X_train)\n",
    "test_std=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler\n",
    "StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X.mean(axis=0))/X.std(axis=0)\n",
    "print(\"Minimum: {}, Maximum: {}\".format(X.min(), X.max()))\n",
    "print(\"Minimum scaled: {}, Maximum scaled: {}\".format(X_scaled.min(), X_scaled.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction \n",
    "Forward selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward selection\n",
    "N_features = 40\n",
    "X_subset = X_scaled.copy()\n",
    "x_names_subset = np.copy(x_names)\n",
    "new_X = []\n",
    "new_X_names = []\n",
    "\n",
    "while len(new_X) < N_features:\n",
    "    r2_list = []\n",
    "    for j in range(X_subset.shape[1]):\n",
    "        model = LinearRegression() #create a linear regression model instance\n",
    "        xj = X_subset[:,j].reshape(-1,1)\n",
    "        model.fit(xj, y) #fit the model\n",
    "        r2 = model.score(xj, y) #get the \"score\", which is equivalent to r^2\n",
    "        r2_list.append([r2, j])\n",
    "    r2_list.sort() #sort lowest to highest\n",
    "    r2_max, j_max = r2_list[-1] #select highest r2 value\n",
    "    new_X.append(X_subset[:,j_max].copy())\n",
    "    new_X_names.append(x_names_subset[j_max])\n",
    "    x_names_subset = np.delete(x_names_subset, j_max)\n",
    "    X_subset = np.delete(X_subset, j_max, axis=1)\n",
    "    \n",
    "print('The {} most linearly correlated features are: {}'.format(N_features, new_X_names))\n",
    "\n",
    "new_X = np.array(new_X).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component regression\n",
    "PCvals, PCvecs = eigvals, eigvecs\n",
    "total_variance = np.sum(np.real(PCvals))\n",
    "explained_variance = np.real(PCvals)/total_variance\n",
    "\n",
    "PC_projection = np.dot(X_scaled, PCvecs)\n",
    "print(PC_projection.shape)\n",
    "\n",
    "corr_PCs = np.corrcoef(PC_projection.T)\n",
    "\n",
    "\n",
    "y = np.array(all_data[real_rows, -3], dtype = 'float')\n",
    "y = y.reshape(-1, 1)\n",
    "model = LinearRegression() #create a linear regression model instance\n",
    "model.fit(PC_projection, y) #fit the model\n",
    "r2 = model.score(PC_projection, y) #get the \"score\", which is equivalent to r^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Receiver Operating Characteristic (ROC) curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "svc = SVC()\n",
    "sgd = SGDClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "sgd.fit(X_blob2, y_blob2)\n",
    "y_sgd = sgd.predict(X_blob2)\n",
    "\n",
    "rf.fit(X_blob2, y_blob2)\n",
    "y_rf = rf.predict(X_blob2)\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_blob2, y_rf)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr)\n",
    "ax.plot(fpr, fpr, '#C0C0C0')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC AUC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X_mc, y_mc = make_blobs(n_samples = 200, centers = 3, cluster_std = 0.5*noisiness, n_features = 2)\n",
    "\n",
    "model = SVC(kernel = 'linear', C = 1, decision_function_shape = 'ovr')\n",
    "\n",
    "model.fit(X_mc, y_mc)\n",
    "y_mc_hat = model.predict(X_mc)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X_mc[:, 0], X_mc[:, 1], c = clrs[y_mc])\n",
    "\n",
    "x_min, x_max = X_mc[:, 0].min() - 1, X_mc[:, 0].max() + 1\n",
    "y_min, y_max = X_mc[:, 1].min() - 1, X_mc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha = 0.4)\n",
    "axes[1].scatter(X_mc[:, 0], X_mc[:, 1], c = clrs[y_mc_hat])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].set_title('Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muticlass classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_cost(w, X, y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,w)\n",
    "    return sum(np.maximum(0, -y*Xb))\n",
    "\n",
    "print(max_cost(w, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_wrong(w, X = X, y = y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,w)\n",
    "    return sum(np.maximum(0, np.sign(-y*Xb)))\n",
    "\n",
    "print(n_wrong(w,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(n_wrong, w)\n",
    "\n",
    "w_count = result.x\n",
    "print(n_wrong(w_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: What are some differences between these two loss functions?\n",
    "The max cost function tells how far the input is from the discrimination line, while the counting loss function only tells the number of misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized Linear Models\n",
    "\n",
    "Perceptron loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max loss function \n",
    "def max_cost(w, X=X, y=y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept,w)\n",
    "    return sum(np.maximum(0, -y*Xb))\n",
    "\n",
    "print(max_cost(w,X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron \n",
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(max_cost, w)\n",
    "w_perceptron = result.x\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot perceptron\n",
    "\n",
    "prediction = linear_classifier(X, w_perceptron)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c = clrs[y_blob + 1])\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[prediction + 1])\n",
    "\n",
    "#plot line\n",
    "m = -w_perceptron[1] / w_perceptron[2]\n",
    "b = -w_perceptron[0] / w_perceptron[2]\n",
    "axes[1].plot(X[:, 0], m*X[:, 0] + b, ls = '-')\n",
    "\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].set_title('Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression\n",
    " implementation of softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.predict(X[:2, :])\n",
    "array([0, 0])\n",
    "\n",
    "clf.predict_proba(X[:2, :])\n",
    "#array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
    " #      [9.7...e-01, 2.8...e-02, ...e-08]])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cost(w, X = X, y = y):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept, w)\n",
    "    exp_yXb = np.exp(-y * Xb)\n",
    "    return sum(np.log(1 + exp_yXb))\n",
    "\n",
    "print(softmax_cost(w, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(softmax_cost, w, args = (X, y))\n",
    "w_logit = result.x\n",
    "\n",
    "prediction = linear_classifier(X, w_logit)\n",
    "\n",
    "#plot line\n",
    "m = -w_logit[1] / w_logit[2]\n",
    "b = -w_logit[0] / w_logit[2]\n",
    "axes[1].plot(X[:, 0], m*X[:, 0] + b, ls = '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_cost(w, X = X, y = y, alpha = 1):\n",
    "    X_intercept = add_intercept(X)\n",
    "    Xb = np.dot(X_intercept, w)\n",
    "    cost = sum(np.maximum(0, 1 - y*Xb))\n",
    "    cost += alpha*np.linalg.norm(w[1:], 2)\n",
    "    return cost\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "w_guess = np.array([-10, -4, -10])\n",
    "result = minimize(regularized_cost, w_guess, args = (X, y, 1))\n",
    "w_svm = result.x\n",
    "\n",
    "prediction = linear_classifier(X, w_svm)\n",
    "#plot line\n",
    "m = -w_svm[1] / w_svm[2]\n",
    "b = -w_svm[0] / w_svm[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linearity and Kernels¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform X to get better classification result at high D \n",
    "X_new = np.exp(-(X[:, 0]**2 + X[:, 1]**2))\n",
    "X_new = X_new.reshape(-1, 1)\n",
    "X_nonlinear = np.append(X, X_new, 1)\n",
    "\n",
    "result = minimize(regularized_cost, w_guess, args = (X_nonlinear, y, 1))\n",
    "w_svm = result.x\n",
    "\n",
    "prediction = linear_classifier(X_nonlinear, w_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel transformation of feature X to get better classification results \n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "X_kernel = rbf_kernel(X, X, gamma=1)\n",
    "print(X_kernel.shape)\n",
    "\n",
    "w_guess = np.zeros(X.shape[0] + 1)\n",
    "\n",
    "result = minimize(regularized_cost, w_guess, args=(X_kernel, y, 1))\n",
    "w_svm = result.x\n",
    "\n",
    "prediction = linear_classifier(X_kernel, w_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector classfier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "model = SVC(kernel = 'rbf', gamma = 1, C = 1000)\n",
    "model.fit(X, y)\n",
    "y_predict = model.predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c = clrs[y])\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[y_predict]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel = 'rbf', gamma = 1, C = Ci)\n",
    "model.fit(X, y)\n",
    "y_predict = model.predict(X)\n",
    "plot_svc_decision_function(model, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 20)\n",
    "knn.fit(X, y)\n",
    "y_predict = knn.predict(X)\n",
    "\n",
    "\n",
    "#plot the boundries \n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c = clrs[y])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha = 0.4)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[y_predict])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].set_title('kNN Prediction (k = 20)');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NB = GaussianNB()\n",
    "NB.fit(X, y)\n",
    "y_predict = NB.predict(X)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c = clrs[y])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = NB.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha = 0.4)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[y_predict])\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].set_title('Naive Bayes Prediction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth = 3)\n",
    "dtree.fit(X_train,y_train)\n",
    "y_predict = dtree.predict(X_train)\n",
    "\n",
    "cm_train = confusion_matrix(y_train, y_predict)\n",
    "\n",
    "y_predict = dtree.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (12, 6))\n",
    "sns.heatmap(cm_train, annot = True, cbar = False, linewidth = .5, ax = axes[0], fmt = 'd')\n",
    "sns.heatmap(cm_test, annot = True, cbar = False, linewidth = .5, ax = axes[1], fmt = 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_mc\n",
    "y = y_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X, y)\n",
    "y_tree = tree.predict(X)\n",
    "\n",
    "#plot boundaries \n",
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c = clrs[y])\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "axes[1].contourf(xx, yy, Z, alpha = 0.4)\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[y_tree])\n",
    "\n",
    "\n",
    "bottom, top = axes[0].get_ylim()\n",
    "axes[1].set_ylim(bottom, top)\n",
    "\n",
    "left, right = axes[0].get_xlim()\n",
    "axes[1].set_xlim(left, right)\n",
    "\n",
    "axes[0].set_title('Original Data')\n",
    "axes[1].set_title('Decision Tree Prediction');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visilization of the tree\n",
    "from io import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(tree, out_file = dot_data,  \n",
    "                filled = True, rounded = True,\n",
    "                special_characters = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensional classificaiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "X_kernel = rbf_kernel(X_perov, X_perov, gamma = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train svm with original data without kernel \n",
    "\n",
    "w_guess = np.array([-10, -4, -10])\n",
    "result = minimize(regularized_cost, w_guess, args = (X_perov[:, 3:5], y_perov, 1))\n",
    "w_svm = result.x\n",
    "\n",
    "\n",
    "prediction = linear_classifier(X_perov[:, 3:5], w_svm)\n",
    "prediction = 2 * prediction - 1\n",
    "\n",
    "# plot boundaries \n",
    "\n",
    "m = -w_svm[1] / w_svm[2]\n",
    "b = -w_svm[0] / w_svm[2]\n",
    "axes[1].plot(X_perov[:, 3], m * X_perov[:, 3] + b, ls = '-')\n",
    "\n",
    "# accuracy evaluation\n",
    "accuracy_score(y_perov, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train svm with kernel \n",
    "w_guess = np.array([-10, -4, -10])\n",
    "result = minimize(regularized_cost, w_guess, args = (X_kernel[:, 3:5], y_perov, 1))\n",
    "w_svm = result.x\n",
    "# plot/accuracy as shown above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel = 'rbf', gamma = 100, C = 1000)\n",
    "model.fit(X_perov[:, 3:5], y_perov)\n",
    "y_predict = model.predict(X_perov[:, 3:5])\n",
    "\n",
    "print(model.score(X_perov[:, 3:5], y_perov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svc with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train) #Shuffle everything just for good measure\n",
    "\n",
    "sigmas = np.array([1e-3, 1e-2, 1e-1, 1, 10, 100])\n",
    "gammas = 1. / 2 / sigmas**2\n",
    "\n",
    "alphas = np.array([1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1])\n",
    "Cs = 1 / alphas\n",
    "\n",
    "parameter_ranges = {'C': Cs, 'gamma': gammas}\n",
    "\n",
    "svc = SVC(kernel = 'rbf')\n",
    "\n",
    "svc_search = GridSearchCV(svc, parameter_ranges, cv = 3)\n",
    "svc_search.fit(X_train, y_train)\n",
    "svc_search.best_estimator_, svc_search.best_score_\n",
    "\n",
    "\n",
    "best_svc = svc_search.best_estimator_\n",
    "\n",
    "y_predict = best_svc.predict(X_test)\n",
    "\n",
    "best_svc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7, 7))\n",
    "sns.heatmap(cm, annot = True, linewidth = .45, cbar = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High Dimentsion data \"summary statistics\" (mean, standard deviation, min, max, etc.) of each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_mnist.mean(axis=0)\n",
    "print(means.shape)\n",
    "means = means.reshape(1,-1) #convert to *row* vector\n",
    "show_image(means, 0)\n",
    "plt.title('Mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdevs = X_mnist.std(axis=0).reshape(1, -1)\n",
    "show_image(stdevs, 0)\n",
    "plt.title('Standard Deviation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = X_dow.mean(axis = 0) # axis=0 means rows \n",
    "stds = X_dow.std(axis = 0)\n",
    "\n",
    "data = pd.DataFrame(means, index = df.columns[1:-5], columns = ['mean'])\n",
    "data['std'] = stds\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram plots (High Dimensional data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_mnist.shape[-1]\n",
    "n = int(np.sqrt(N)) #n = 8 here\n",
    "fig, axes = plt.subplots(n, n, figsize = (5 * n, 5 * n), dpi = 200)\n",
    "ax_list = axes.ravel()\n",
    "for i in range(N):\n",
    "    ax_list[i].hist(X_mnist[:, i])\n",
    "    ax_list[i].set_xlabel(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "krr = KernelRidge(kernel = 'rbf')\n",
    "alphas = np.logspace(-4, -1, 4)\n",
    "gammas = np.logspace(-6, -3, 4)\n",
    "param_grid = {'alpha': alphas, 'gamma': gammas}\n",
    "\n",
    "krr_search = GridSearchCV(krr, param_grid, cv = 3)\n",
    "krr_search.fit(pls_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y_dow)\n",
    "r2_LR = model.score(X, y_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "linreg.score(X_test, y_test)\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_squared_train, y_train)\n",
    "r2_train = linreg.score(X_squared_train, y_train)\n",
    "r2_test = linreg.score(X_squared_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC model and GridSearchCV method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_perov, y_perov, test_size = 0.4)\n",
    "\n",
    "parameters = {\"C\": C_range, \"gamma\": gamma_range}\n",
    "\n",
    "classifier = SVC(kernel = 'rbf')\n",
    "\n",
    "clf = GridSearchCV(classifier, parameters)\n",
    "clf.fit(X_train_regular,y_train)\n",
    "print(clf.best_estimator_.score(X_test_regular, y_test))\n",
    "svc_reg = clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensional reduction \n",
    "\n",
    "The \"stress\" function compares the distance between points $i$ and $j$ in a low-dimensional space to the distance in the full-dimensional space:\n",
    "\n",
    "$ S(\\vec{x}_{0}, \\vec{x}_1, \\vec{x}_2, ... \\vec{x}_n) =  \\left( \\frac{\\sum_{i=0}^n \\sum_{i<j}(d_{ij} - ||x_i - x_j||)^2}{\\sum_{i=0}^n \\sum_{i<j} d_{ij}^2} \\right)^{1/2} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "def stress(X_reduced, X):\n",
    "    D_red = pdist(X_reduced)\n",
    "    D_tot = pdist(X)\n",
    "    numerator = np.sum((D_tot - D_red)**2)\n",
    "    denom = np.sum(D_tot**2)\n",
    "    return np.sqrt(numerator / denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "pca = PCA(n_components = n_components)\n",
    "%time X_pca = pca.fit_transform(X)\n",
    "\n",
    "kpca = KernelPCA(n_components = n_components, kernel = 'rbf', gamma = 0.1)\n",
    "%time X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "tsne = TSNE(n_components = n_components)\n",
    "%time X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the rank and eigenvaluse of the corvariance matrix\n",
    "C = np.cov(X_mnist.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(C)\n",
    "eig_vecs = eig_vecs.T #<- note that the eigenvectors are the *columns* by default\n",
    "print('Rank of the covariance matrix: {}'.format(np.linalg.matrix_rank(C)))\n",
    "print(eig_vals)\n",
    "\n",
    "sorted_idxs = np.argsort(eig_vals) #this gives us the list of indices from smallest to largest\n",
    "sorted_idxs = list(sorted_idxs)\n",
    "sorted_idxs.reverse() #this goes from largest to smallest\n",
    "eig_vals = eig_vals[sorted_idxs] #re-sort values\n",
    "eig_vecs = eig_vecs[sorted_idxs, :] #re-sort vectors\n",
    "\n",
    "# project on k dimensions\n",
    "k = 2\n",
    "projector = eig_vecs[:k, :].T\n",
    "X_k = np.dot(X_mnist, projector)\n",
    "X_reconstructed = np.dot(projector, X_k.T).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "k=9\n",
    "pca_model = PCA(n_components = k)\n",
    "pca_model.fit(X_mnist)\n",
    "X_pca = pca_model.transform(X_mnist)a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "k = 2\n",
    "gamma = 10\n",
    "lPCA = PCA(n_components = k)\n",
    "kPCA = KernelPCA(n_components = k, kernel = 'rbf', gamma = gamma, fit_inverse_transform = True)\n",
    "\n",
    "lPCA.fit(X_m)\n",
    "X_PCA = lPCA.transform(X_m)\n",
    "\n",
    "kPCA.fit(X_m)\n",
    "X_kPCA = kPCA.transform(X_m)\n",
    "\n",
    "# PCA invertion to high-d\n",
    "X_PCA_reconstruct = lPCA.inverse_transform(X_PCA)\n",
    "X_kPCA_reconstruct = kPCA.inverse_transform(X_kPCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold learning\n",
    "MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "k = 2\n",
    "mds = MDS(n_components = k, n_init = 1, max_iter = 100) #<- note that we need to give some max_iteration and initial guess parameters since this is iterative\n",
    "X_mds = mds.fit_transform(X_mnist) #<- note that there is no transform method. What does this mean?\n",
    "\n",
    "# the goal is to minimize the stress function \n",
    "stress(X_mds, X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold based tSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components = 2, perplexity = 30.0, \n",
    "            early_exaggeration = 12.0, \n",
    "            learning_rate = 200.0, \n",
    "            n_iter = 1000,\n",
    "            init = 'random',\n",
    "            method = 'exact')\n",
    "\n",
    "X_tsne = tsne.fit_transform(X_mnist)\n",
    "\n",
    "stress(X_tsne, X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering: unsupervised algorithms\n",
    "\n",
    "Clustering algorithms seek to identify data points that are similar to each other based on a set of descriptive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means (clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "random_state = 20\n",
    "X = X_pca #scikit-learn is much more efficient, so we can run it on the whole datset\n",
    "\n",
    "model = KMeans(n_clusters = n_clusters)#, random_state=random_state)\n",
    "model.fit(X)\n",
    "y_predict = model.predict(X)\n",
    "centers = model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "X_i = X_pca\n",
    "n_clusters = 10\n",
    "\n",
    "model = GaussianMixture(n_components = n_clusters, random_state = random_state, covariance_type = covariance_type)\n",
    "model.fit(X_i)\n",
    "y_predict = model.predict(X_i)\n",
    "\n",
    "centers = model.means_\n",
    "\n",
    "silhouette = silhouette_score(X_i, y_predict)\n",
    "c_h_score = calinski_harabasz_score(X_i, y_predict)\n",
    "BIC=model.bic(X_i)\n",
    "\n",
    "print(silhouette)\n",
    "print(c_h_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "n_clusters = 2\n",
    "random_state = 0\n",
    "covariance_type = 'full' #full, tied, spherical\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (15, 4))\n",
    "\n",
    "for X_i, label, ax in zip(data, labels, axes):\n",
    "    model = GaussianMixture(n_components = n_clusters, random_state = random_state, covariance_type = covariance_type)\n",
    "    model.fit(X_i)\n",
    "    y_predict = model.predict(X_i)\n",
    "    centers = model.means_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density-based models\n",
    "\n",
    "Density-based clustering algorithms consider local density of points and utilize this information to group points into clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "model = MeanShift(bandwidth = 21)\n",
    "%time model.fit(X_tsne)\n",
    "labels = model.labels_\n",
    "centroids = model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = X_pca\n",
    "\n",
    "model = DBSCAN(eps = 1, min_samples = 3)\n",
    "y_predict = model.fit_predict(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical models:These models construct linkages between different points and use distance cutoffs to assign clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "X = X_pca\n",
    "\n",
    "Z = linkage(X, method='single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"cophenetic coefficient\" measures the ratio of the distance in \"linkage\" space to the distance in the high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "Dij = pdist(X, metric = 'euclidean')\n",
    "for method in ['single', 'complete', 'average', 'weighted', 'centroid', 'ward']:\n",
    "    Z = linkage(X, method = method)\n",
    "    C, coph_dists = cophenet(Z, Dij)\n",
    "    print('cophenetic coefficient of {}: {}'.format(method, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dendrogram function is a visual representation of this \"linkage\" structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (15, 6))\n",
    "Z = linkage(X, method = 'centroid')\n",
    "dendrogram(Z, color_threshold = 20, ax = axes[1])\n",
    "axes[0].scatter(X[:, 0], X[:, 1])\n",
    "axes[0].set_title('PCA Data')\n",
    "axes[1].set_title('Dendrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "max_d = 20\n",
    "k = 4\n",
    "Z = linkage(X, method = 'centroid')\n",
    "\n",
    "clusters_dist = fcluster(Z, max_d, criterion = 'distance')\n",
    "clusters_k = fcluster(Z, k, criterion = 'maxclust')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n",
    "dendrogram(Z, color_threshold = max_d, truncate_mode = 'lastp', p = k, ax = axes[0])\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[clusters_dist])\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c = clrs[clusters_k])\n",
    "\n",
    "axes[0].set_title('Truncated Dendrogram')\n",
    "axes[1].set_title(\"Agglomerative Clustering w/ criterion = 'distance'\")\n",
    "axes[2].set_title(\"Agglomerative Clustering w/ criterion = 'maxclust'\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_cutoff = 9\n",
    "clusters_I = fcluster(Z, I_cutoff, criterion = 'inconsistent', depth = 10)\n",
    "n_clusters = max(clusters_I)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize = (12, 10))\n",
    "axes = axes.ravel()\n",
    "dendrogram(Z, color_threshold = 3, truncate_mode = 'lastp', p = int(n_clusters), ax = axes[0])\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c = clrs[clusters_I])\n",
    "axes[2].scatter(X[:, 0], X[:, 1], c = clrs[clusters_dist])\n",
    "axes[3].scatter(X[:, 0], X[:, 1], c = clrs[clusters_k])\n",
    "\n",
    "axes[0].set_title('Truncated Dendrogram')\n",
    "axes[1].set_title(\"Agglomerative Clustering w/ criterion = 'inconsistent'\");\n",
    "axes[2].set_title(\"Agglomerative Clustering w/ criterion = 'distance'\")\n",
    "axes[3].set_title(\"Agglomerative Clustering w/ criterion = 'maxclust'\")\n",
    "\n",
    "print('Number of clusters:', n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generative Model：Generative models describe the probability distribution of the underlying data. They can be used to explore datasets in many different ways, and are unsupervised since they do not require labels for the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu = 0\n",
    "variance = 1\n",
    "sigma = np.sqrt(variance)\n",
    "x = np.linspace(mu - 3 * sigma, mu + 3 * sigma, 100)\n",
    "gauss = norm.pdf(x, mu, sigma)\n",
    "X_new = norm.rvs(mu, sigma, size = 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 6\n",
    "x_1d = X_dow[:, feature]\n",
    "\n",
    "mu = x_1d.mean()\n",
    "std = x_1d.std()\n",
    "\n",
    "x_synthetic = norm.rvs(mu, std, size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_A = 6\n",
    "feature_B = 4\n",
    "X_2d = X_dow[:, [feature_A, feature_B]]\n",
    "\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "N_clusters = 2\n",
    "gmm = GaussianMixture(n_components = N_clusters, covariance_type = 'full', random_state = 0)\n",
    "gmm.fit(X_2d)\n",
    "y_2d = gmm.predict(X_2d)\n",
    "bic = gmm_n.bic(X_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(2, 50)[::3]\n",
    "\n",
    "BICs = []\n",
    "for n in n_components:\n",
    "    gmm_n =  GaussianMixture(n, covariance_type = 'full').fit(X_2d)\n",
    "    bic = gmm_n.bic(X_2d)\n",
    "    BICs.append(bic)\n",
    "    models.append(gmm_n)\n",
    "    \n",
    "min_idx = BICs.index(min(BICs))\n",
    "gmm_best = models[min_idx]\n",
    "example = gmm_best.sample()\n",
    "show_image(example, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist_6 = X_mnist[y_mnist == 6]\n",
    "\n",
    "# Let's just use an arbitrary model\n",
    "gmm_n =  GaussianMixture(5, covariance_type = 'spherical').fit(X_mnist_6)\n",
    "example = gmm_n.sample()\n",
    "show_image(example, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# instantiate and fit the KDE model\n",
    "x_1d = x_1d.reshape(-1, 1)\n",
    "kde = KernelDensity(bandwidth = 0.15, kernel = 'gaussian')\n",
    "kde.fit(x_1d)\n",
    "\n",
    "#create a continuous x variable\n",
    "x_continuous = np.linspace(min(x_1d), max(x_1d), 1000)\n",
    "\n",
    "# score_samples returns the log of the probability density\n",
    "logprob = kde.score_samples(x_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_synthetic = kde.sample(10000)\n",
    "X_synthetic = X_synthetic.reshape(-1, 1)\n",
    "kde = KernelDensity(bandwidth = 0.15, kernel = 'gaussian')\n",
    "kde.fit(X_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not-so-navie bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0\n",
    "X = X_mnist[y_mnist == label]\n",
    "\n",
    "model = KernelDensity(bandwidth = 10, kernel = 'gaussian')\n",
    "model.fit(X);\n",
    "\n",
    "model.score_samples(X_mnist[:3, :])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_mnist, y_mnist, test_size = 0.3, random_state = 1)\n",
    "prediction = not_so_naive(X_train, X_test, y_train, model)\n",
    "\n",
    "accuracy_score(y_test, prediction)\n",
    "cm = confusion_matrix(y_test.reshape(-1,), prediction)\n",
    "df_cm = pd.DataFrame(cm, index = range(0, 10), columns = range(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = GaussianNB()\n",
    "yhat = NB.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "NB.score(X_test, y_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, yhat)\n",
    "df_cm = pd.DataFrame(cm, index = range(10), columns = range(10))\n",
    "sns.heatmap(df_cm, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLS (Feature Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a KRR model based on the first 5 PLS components\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_dow, test_size = 0.3)\n",
    "\n",
    "pls = PLSRegression(n_components = 5)\n",
    "pls.fit(X_train, y_train)\n",
    "pls_train = pls.transform(X_train)\n",
    "pls_test = pls.transform(X_test)\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "krr = KernelRidge(kernel = 'rbf')\n",
    "alphas = np.logspace(-4, -1, 4)\n",
    "gammas = np.logspace(-6, -3, 4)\n",
    "param_grid = {'alpha': alphas, 'gamma': gammas}\n",
    "\n",
    "krr_search = GridSearchCV(krr, param_grid, cv = 3)\n",
    "krr_search.fit(pls_train, y_train)\n",
    "krr_search.best_estimator_.score(pls_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "r2s_PLS = []\n",
    "m_PLS = range(1, X.shape[1]) #PLS does not allow more components than original features\n",
    "for m in m_PLS:\n",
    "    model = PLSRegression(n_components = m)\n",
    "    model.fit(X, y_dow)\n",
    "    r2 = model.score(X, y_dow)\n",
    "    r2s_PLS.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X_blobs, y_blobs = make_blobs(n_samples = 50, centers = 2, cluster_std = 0.5, n_features = 2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(\"Digits data shape: {}\".format(digits.data.shape))\n",
    "print(\"Digits output shape: {}\".format(digits.target.shape))\n",
    "X_mnist = np.array(digits.data)\n",
    "y_mnist = np.array(digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear discriminator analysis LDA (feature transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_mnist, y_mnist)\n",
    "X_LDA = lda.transform(X_mnist)\n",
    "\n",
    "print(X_LDA.shape)\n",
    "\n",
    "W_lda = lda.scalings_ # extract weights or transform the X matrix directly with scikit learn\n",
    "X_lda = linear_combination(X_mnist, W_lda)\n",
    "print(X_lda.shape)\n",
    "\n",
    "score = lda.score(X_test, y_test)\n",
    "print(score)\n",
    "X_test_LDA = lda.transform(X_test)\n",
    "X_train_LDA = lda.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (feature transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "k=9\n",
    "pca_model = PCA(n_components = k)\n",
    "pca_model.fit(X_mnist)\n",
    "X_pca = pca_model.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = lda.predict(X_test)\n",
    "CM = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "df_cm = pd.DataFrame(CM, index = range(0, 10), columns = range(0, 10))\n",
    "sns.heatmap(df_cm, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha = 1.0)\n",
    "model.fit(X_squared_train, y_train)\n",
    "r2_train = model.score(X_squared_train, y_train)\n",
    "r2_test = model.score(X_squared_test, y_test)\n",
    "\n",
    "coeffs = model.coef_\n",
    "print(\"Total Number of Coefficients: {}\".format(len(coeffs)))\n",
    "nonzero_coeffs = [c for c in coeffs if abs(c) > 0]\n",
    "print(\"Number of Non-Zero Coefficients: {}\".format(len(nonzero_coeffs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autofeat and symbolic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install autofeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autofeat import AutoFeatRegressor\n",
    "transforms = ['1/', 'exp', 'abs', 'sqrt', '^2', '^3']\n",
    "afreg = AutoFeatRegressor(verbose = 1, feateng_steps = 2, featsel_runs = 1, transformations = transforms)\n",
    "afreg.fit(X_train, y_train)\n",
    "\n",
    "afreg.new_feat_cols_\n",
    "afreg.good_cols_   # all of the features that were selected by the feature selection algorithm\n",
    "\n",
    "afreg.score(X_test, y_test)\n",
    "\n",
    "X_afreg_train = afreg.transform(X_train) # creat new feature matrix contains all new features\n",
    "X_afreg_test = afreg.transform(X_test)\n",
    "\n",
    "linreg.fit(X_afreg_train, y_train)\n",
    "linreg.score(X_afreg_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_input_names = dow_feature_names[:X_train.shape[1]]\n",
    "X_train_df = pd.DataFrame(X_train, columns = dow_input_names)\n",
    "X_test_df = pd.DataFrame(X_test, columns = dow_input_names)\n",
    "\n",
    "transforms = [\"1/\", \"exp\", \"log\", \"abs\", \"sqrt\", \"^2\", \"^3\", \"1+\", \"1-\", \"exp-\"]\n",
    "afreg = AutoFeatRegressor(verbose = 1, feateng_steps = 2, featsel_runs = 1, transformations = transforms, units = unit_dict)\n",
    "afreg.fit(X_train_df, y_train)\n",
    "\n",
    "afreg.score(X_test_df, y_test)\n",
    "\n",
    "afreg.new_feat_cols_ # features generated\n",
    "afreg.good_cols_ # features that have been selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time series \n",
    "statsmodels packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! conda install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as api\n",
    "\n",
    "sm_data = api.datasets.co2.load_pandas()\n",
    "co2_df = sm_data.data\n",
    "co2_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data in time series analysis:forward fill/back fill/linear interp/spline interp/linear interp\n",
    "fillna and interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_df = co2_df\n",
    "interp_df['forward_fill'] = interp_df['co2'].fillna(method='ffill')\n",
    "interp_df['back_fill'] = interp_df['co2'].fillna(method='bfill')\n",
    "interp_df['linear_interp'] = interp_df['co2'].interpolate(method='linear')\n",
    "interp_df['spline_interp'] = interp_df['co2'].interpolate(method='spline', order=3)\n",
    "\n",
    "interp_df.plot();\n",
    "\n",
    "co2_df = interp_df[['co2', 'linear_interp']]\n",
    "co2_df = co2_df.rename(columns = {'linear_interp': 'co2_interp'})\n",
    "co2_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving Average Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panda as pd \n",
    "window = 10\n",
    "\n",
    "rolling_df = co2_df['co2_interp'].rolling(window)\n",
    "\n",
    "rolling_df\n",
    "\n",
    "moving_avg = rolling_df.mean()\n",
    "moving_avg.plot();\n",
    "\n",
    "moving_stdev = rolling_df.std()\n",
    "moving_stdev.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_dow_df = dow_df.rolling(24)\n",
    "moving_avg_dow = rolling_dow_df.mean()\n",
    "moving_avg_dow.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import polyfit\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "lag = 20\n",
    "dataset = co2_df['co2_interp']\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for i in range(len(dataset)):\n",
    "    if i >= lag:\n",
    "        x_i = dataset[i]\n",
    "        x_lag = dataset[i - lag]\n",
    "        xs.append(x_lag)\n",
    "        ys.append(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the autocorrelation using the `statsmodels` package:\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "autocorr = acf(dataset, nlags = 40)\n",
    "print(len(autocorr))\n",
    "print(autocorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note the _ = syntax is just used to avoid displaying the same plot twice (a small issue with how statsmodels works in Jupyter notebooks).:\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "_ = plot_acf(dataset, lags = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial autocorrelation\n",
    "_ = plot_pacf(dataset, lags = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_acf(dow_df, lags = 100)\n",
    "_ = plot_pacf(dow_df, lags = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationary data\n",
    "Statistical test: a test for determining if a dataset is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "p_val = adfuller(co2_df['co2_interp'])[1]\n",
    "print(\"Probability the data is stationary: {}\".format(1 - p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differencing \n",
    "Dickey-Fuller test to determine if data is stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_df['co2_diff'] = co2_df['co2_interp'] - co2_df['co2_interp'].shift(1)\n",
    "co2_df['co2_diff'].plot();\n",
    "\n",
    "p_val = adfuller(co2_df['co2_diff'][1:])[1]\n",
    "print(\"Probability the data is stationary: {}\".format(1 - p_val))\n",
    "\n",
    "_ = plot_acf(co2_df['co2_diff'][1:], lags = 100)\n",
    "\n",
    "_ = plot_pacf(co2_df['co2_diff'][1:], lags = 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "dow_df['diff'] = dow_df['y:Impurity'] - dow_df['y:Impurity'].shift(1)\n",
    "\n",
    "p_val = adfuller(dow_df['diff'][1:])[1]\n",
    "print(\"Probability the data is stationary: {}\".format(1 - p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fitting to remove seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = co2_df['co2_interp']\n",
    "weeks = np.arange(0, len(y))\n",
    "m, b = np.polyfit(weeks, y, deg = 1)\n",
    "yhat_linear = m*weeks + b\n",
    "resid = y - yhat_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we will use LASSO to identify the most highly-correlated frequencies/offsets:\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha = 0.1)\n",
    "model.fit(X, y)\n",
    "yhat = model.predict(X)\n",
    "model_resid = y - yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if data is stationary \n",
    "p_val = adfuller(model_resid)[1]\n",
    "print(\"Probability the data is stationary: {}\".format(1 - p_val))\n",
    "\n",
    "_ = plot_acf(model_resid, lags = 104)\n",
    "_ = plot_pacf(model_resid, lags = 104)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply differecing in addiiton to model fitting to remove these correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resid_diff = model_resid - model_resid.shift(1)\n",
    "model_resid_diff = model_resid_diff[1:] #remove NaN from 0th position\n",
    "\n",
    "p_val = adfuller(model_resid_diff)[1]\n",
    "print(\"Probability the data is stationary: {}\".format(1 - p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ARIMA Modeling :combine the auto-regressive models with two additional terms:Integration and Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df_model = co2_df.copy()\n",
    "\n",
    "del df_model['co2']\n",
    "del df_model['co2_diff']\n",
    "\n",
    "# make test/train split\n",
    "train = df_model[:N_train]\n",
    "test = df_model[N_train:N_train + N_test]\n",
    "\n",
    "# get d,p,q value d= probability of differencing; p= partial autocorrelation peaks above 0,\n",
    "#q= autocorrelation peaks above 0\n",
    "diffed = co2_df['co2_interp'] - co2_df['co2_interp'].shift(1)\n",
    "diffed = diffed[1:]\n",
    "\n",
    "p_val = adfuller(diffed)[1]\n",
    "print(\"Probability the data is stationary after 1 difference: {}\".format(1 - p_val))\n",
    "\n",
    "_ = plot_acf(diffed)\n",
    "_ = plot_pacf(diffed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "model = ARIMA(train, order=(4, 1, 4)) # order p,d,q\n",
    "model_fit = model.fit(disp = 0)\n",
    "print(model_fit.summary())\n",
    "\n",
    "model_fit.plot_predict(dynamic = False)\n",
    "print(np.mean(np.abs(model_fit.resid)))\n",
    "plt.show()\n",
    "\n",
    "# Forecast\n",
    "fc, se, conf = model_fit.forecast(N_test, alpha = 0.05)  # 95% conf\n",
    "\n",
    "# Make as pandas series\n",
    "fc_series = pd.Series(fc, index = test.index)\n",
    "lower_series = pd.Series(conf[:, 0], index = test.index)\n",
    "upper_series = pd.Series(conf[:, 1], index = test.index)\n",
    "\n",
    "# Plot\n",
    "plt.plot(train, label = 'Training Data')\n",
    "plt.plot(test, label = 'Test Data')\n",
    "plt.plot(fc_series, label = 'Forecast')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, alpha=.15)\n",
    "plt.title('Forecast vs Actual Data')\n",
    "plt.legend(loc='upper left', fontsize = 8);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Any `assert` statements are provided to check your answers. You may receive partial credit even if these code blocks fail, and you may not receive full credit even if they pass (e.g. if you hack them to pass). Problems without `assert` statements may have multiple correct solutions, and will be manually graded by instructors.\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE`, and delete any line that says `raise NotImplementedError`. Please also your name and official GT ID below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Liyun Ren\"\n",
    "GTID = \"lren42\" #e.g. gburdell0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2620cbcec159904c7a5cf90a677bc329",
     "grade": false,
     "grade_id": "cell-ddedf132b958be28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Problem 1: Kernel Regression\n",
    "\n",
    "In this problem you will work with a well-known example dataset of housing prices in Boston. You will train and test a kernel regression model for this dataset using an RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #ignore warnings that may arise. A bad idea in general, but fine for this exam.\n",
    "\n",
    "#load dataset\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56213cf9a54abae397964b79a2a67acc",
     "grade": false,
     "grade_id": "cell-21788733a1891281",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1a: Feature Scaling\n",
    "\n",
    "Apply \"standard scaling\" to this dataset, resulting in a re-scaled data matrix, `X_rescaled` where the mean and standard devation of each feature is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4173f0910200b8aa7ba5f8effadcf0c5",
     "grade": false,
     "grade_id": "cell-73e892e89d35ff01",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_rescaled=(X-X.mean(axis=0))/(X.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ... -1.45900038  0.44105193\n",
      "  -1.0755623 ]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ... -0.30309415  0.44105193\n",
      "  -0.49243937]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ... -0.30309415  0.39642699\n",
      "  -1.2087274 ]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.98304761]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  1.17646583  0.4032249\n",
      "  -0.86530163]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  1.17646583  0.44105193\n",
      "  -0.66905833]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "print(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "beaf65b5707c37e415192a7843578e57",
     "grade": true,
     "grade_id": "cell-56625bea3776c638",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(np.linalg.norm(np.mean(X_rescaled, axis=0)), 0)\n",
    "assert np.isclose(np.std(X_rescaled, axis=0), np.ones(np.size(np.std(X_rescaled, axis=0)))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "126aa6bdf42c3b71435e0630160163b9",
     "grade": false,
     "grade_id": "cell-f8d8de1a3f3e08e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1b: Construct an RBF kernel\n",
    "\n",
    "Use every 10th data point from the rescaled data matrix to construct a radial basis function kernel matrix with `gamma = 0.1`. Name your kernel matrix `K_10th`. If you were unable to construct the re-scaled matrix you can use the original data matrix instead.\n",
    "\n",
    "Recall that the formula for an RBF kernel is:\n",
    "\n",
    "$K_{RBF}(i, j) = exp(-\\gamma ||\\vec{x}_i - \\vec{x}_j||^2)$\n",
    "\n",
    "where $\\vec{x}_i$ is the $i^{th}$ data point and $|| \\vec{a} ||$ represents the 2-norm of $\\vec{a}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4e05f74a94e078a30baf772f4d91726",
     "grade": false,
     "grade_id": "cell-3f3e5559946a62e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_10th=X_scaled[::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF(X_train,X_predict,gamma=0.1):\n",
    "    K=np.zeros((X_train.shape[0],X_predict.shape[0]))\n",
    "    for i in range(K.shape[0]):\n",
    "        for j in range(K.shape[1]):\n",
    "            K[i,j]=np.exp(-gamma*(np.linalg.norm(X_predict[j]-X_train[i]))**2)\n",
    "    return K\n",
    "K_10th=RBF(X_10th,X_10th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf(X_10th, x_test = None, gamma = 0.1):\n",
    "    if x_test is None:\n",
    "        x_test = X_10th       \n",
    "    N = len(x_test)\n",
    "    M = len(X_10th)\n",
    "    K = np.zeros((N, M))\n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            K[i, j] = np.exp(-gamma * np.linalg.norm((x_test[i] - X_10th[j]))**2)  \n",
    "            #   whenever told to use norm, make sure to use np.linalg.norm    \n",
    "    return K\n",
    "K_10th = rbf(X_10th, gamma = 0.1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46dff912f814a7df525bf74a01aae285",
     "grade": true,
     "grade_id": "cell-f67fcefb53b4e190",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "eigvals, eigvecs = np.linalg.eig(K_10th)\n",
    "assert np.isclose(sum(eigvals[:10]), 36.4834)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52dd2e9104bfed41714bbb536e2d45f2",
     "grade": false,
     "grade_id": "cell-39e0bcb257c19157",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1c: Train and test a kernel regression model.\n",
    "\n",
    "Train a kernel linear regression model using the RBF kernel for every 10th data point. You do not need to add an intercept term. If you were unable to construct the RBF kernel matrix, you may use every 10th point from the original dataset instead. Compute and report the mean absolute error (MAE) on the full dataset after training.\n",
    "\n",
    "For full credit, you should only use the Python standard library and `numpy` in this problem, but partial credit will be awarded if `scikit-learn` functions are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa0194d1978a49f159dd806dc8c1c3b2",
     "grade": true,
     "grade_id": "cell-2954486d8b13c592",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "y_10th=y[::10]\n",
    "A=np.dot(K_10th.T,K_10th)\n",
    "b=np.dot(K_10th.T,y_10th)\n",
    "w=np.linalg.solve(A,b)\n",
    "K_all=RBF(X_10th,X_rescaled)\n",
    "yhat=np.dot(w,K_all)\n",
    "MAE=np.mean(np.abs(y-yhat))\n",
    "print('MAE = {}'.format(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAE(actual, prediction):\n",
    "    z = zip(actual, prediction)\n",
    "    N = len(actual)\n",
    "    sum = 0\n",
    "    for a,p in z:\n",
    "        sum += abs(a - p)\n",
    "    mae = sum / N\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.272744014563692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "y_10th=y[::10]\n",
    "X_test=X[::20]\n",
    "X_train = rbf(X_10th, gamma=0.1)\n",
    "model_rbf = LinearRegression()\n",
    "model_rbf.fit(X_10th, y_10th)\n",
    "yhat_rbf = model_rbf.predict(X_test)\n",
    "mae = MAE(y, yhat_rbf)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85d36804b53eebaddc74ebcdb47459ca",
     "grade": false,
     "grade_id": "cell-07d1301b8cb0611d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1d: Hyperparameter optimization for kernel ridge regression\n",
    "\n",
    "Use the `GridSearchCV` function to determine the optimum hyperparameters for KRR with the Boston housing dataset. You should use the radial basis function kernel, and search over the following range of parameters:\n",
    "\n",
    "$\\alpha \\in [1e-4, 1e-3, 1e-2, 1e-1, 1]$\n",
    "\n",
    "$\\gamma \\in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2]$\n",
    "\n",
    "Report the $r^2$ score of the optimum hyperparamters on a validation set consisting of 30% of the data that is randomly selected from the original dataset, and that is **not used at any point in the hyperparameter training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c1e0c411e2552cbd0284d67d25a96f20",
     "grade": true,
     "grade_id": "cell-88e83fc8e8e6df4e",
     "locked": false,
     "points": 15,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal hyperparameters: alpha = 0.0001, gamma=1e-05\n",
      "Validation R^2=0.786\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split \n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "params={'alpha':[1e-4,1e-3,1e-2,1e-1,1],'gamma':[1e-6,1e-5,1e-4,1e-3,1e-2]}\n",
    "KRR=KernelRidge(kernel='rbf')\n",
    "GSCV=GridSearchCV(estimator=KRR,param_grid=params)\n",
    "GSCV.fit(X_train,y_train)\n",
    "best_KRR=GSCV.best_estimator_\n",
    "print(\"Optimal hyperparameters: alpha = {}, gamma={}\".format(best_KRR.alpha,best_KRR.gamma))\n",
    "r2_val=best_KRR.score(X_test,y_test)\n",
    "print(\"Validation R^2={:.3f}\".format(r2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KernelRidge' object has no attribute 'best_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e17e97dd14ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mKRR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKRR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mKRR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KernelRidge' object has no attribute 'best_estimator'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "alphas = np.array([1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1])\n",
    "sigmas = np.array([5, 10, 15, 20, 25, 30, 35, 40])\n",
    "parameter_ranges = {'alphas': alphas, 'sigmas':sigmas}\n",
    "KRR = KernelRidge(kernel='rbf')\n",
    "KRR_search = GridSearchCV(KRR, parameter_ranges)\n",
    "KRR.fit(x_train, y_train)\n",
    "yhat = KRR.predict(x_test)\n",
    "KRR.best_estimator(x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of assignment. Any code appearing past this point will not be graded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
